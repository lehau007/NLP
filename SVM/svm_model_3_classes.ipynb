{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\", encoding='Latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Sentiment\n",
       "0  The GeoSolutions technology will leverage Bene...  positive\n",
       "1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
       "2  For the last quarter of 2010 , Componenta 's n...  positive\n",
       "3  According to the Finnish-Russian Chamber of Co...   neutral\n",
       "4  The Swedish buyout firm has sold its remaining...   neutral"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5842, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True) #removing all empty spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all sentences to lowercase\n",
    "df['Sentence'] = [entry.lower() for entry in df['Sentence']]\n",
    "\n",
    "mp = {'positive': 2, 'negative': 0, 'neutral': 1}\n",
    "df['Sentiment'] = df['Sentiment'].map(mp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the geosolutions technology will leverage bene...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$esi on lows, down $1.50 to $2.50 bk a real po...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>for the last quarter of 2010 , componenta 's n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>according to the finnish-russian chamber of co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the swedish buyout firm has sold its remaining...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Sentiment\n",
       "0  the geosolutions technology will leverage bene...          2\n",
       "1  $esi on lows, down $1.50 to $2.50 bk a real po...          0\n",
       "2  for the last quarter of 2010 , componenta 's n...          2\n",
       "3  according to the finnish-russian chamber of co...          1\n",
       "4  the swedish buyout firm has sold its remaining...          1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'geosolutions', 'technology', 'will', 'leverage', 'benefon', \"'s\", 'gps', 'solutions', 'by', 'providing', 'location', 'based', 'search', 'technology', ',', 'a', 'communities', 'platform', ',', 'location', 'relevant', 'multimedia', 'content', 'and', 'a', 'new', 'and', 'powerful', 'commercial', 'model', '.']\n"
     ]
    }
   ],
   "source": [
    "sentences = [sen.split() for sen in df[\"Sentence\"]]\n",
    "print(sentences[0]) # like word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no_punctuation, stopWords, stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'geosolutions', 'technology', 'will', 'leverage', 'benefon', 'gps', 'solutions', 'by', 'providing', 'location', 'based', 'search', 'technology', 'a', 'communities', 'platform', 'location', 'relevant', 'multimedia', 'content', 'and', 'a', 'new', 'and', 'powerful', 'commercial', 'model']\n"
     ]
    }
   ],
   "source": [
    "# punc\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word.isalnum()]\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['geosolutions', 'technology', 'leverage', 'benefon', 'gps', 'solutions', 'providing', 'location', 'based', 'search', 'technology', 'communities', 'platform', 'location', 'relevant', 'multimedia', 'content', 'new', 'powerful', 'commercial', 'model']\n"
     ]
    }
   ],
   "source": [
    "# stopWords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stop_words]\n",
    "print(sentences[0])\n",
    "# Output: don't have stop words in sentence[i] any more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['geosolut', 'technolog', 'leverag', 'benefon', 'gp', 'solut', 'provid', 'locat', 'base', 'search', 'technolog', 'commun', 'platform', 'locat', 'relev', 'multimedia', 'content', 'new', 'power', 'commerci', 'model']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [stemmer.stem(word) for word in sentences[i]]\n",
    "\n",
    "print(sentences[0])\n",
    "# Output: running to run, location to locat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "processed_sentences = [' '.join(sentence) for sentence in sentences]\n",
    "\n",
    "# Target variable\n",
    "y = df['Sentiment'].values  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_sentences, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data using TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=15000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(X_train_tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.34%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.15      0.22       175\n",
      "           1       0.71      0.89      0.79       622\n",
      "           2       0.79      0.68      0.73       372\n",
      "\n",
      "    accuracy                           0.71      1169\n",
      "   macro avg       0.64      0.57      0.58      1169\n",
      "weighted avg       0.69      0.71      0.68      1169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the SVM model\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.25%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.39      0.54       685\n",
      "           1       0.80      0.97      0.88      2508\n",
      "           2       0.93      0.84      0.88      1480\n",
      "\n",
      "    accuracy                           0.84      4673\n",
      "   macro avg       0.87      0.73      0.77      4673\n",
      "weighted avg       0.85      0.84      0.83      4673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_model.predict(X_train_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 70.40%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.14      0.22       175\n",
      "           1       0.70      0.89      0.78       622\n",
      "           2       0.76      0.66      0.70       372\n",
      "\n",
      "    accuracy                           0.70      1169\n",
      "   macro avg       0.64      0.56      0.57      1169\n",
      "weighted avg       0.68      0.70      0.67      1169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "log_reg_model = LogisticRegression(max_iter=1000)\n",
    "log_reg_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 66.04%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.16      0.19       175\n",
      "           1       0.70      0.77      0.74       622\n",
      "           2       0.71      0.71      0.71       372\n",
      "\n",
      "    accuracy                           0.66      1169\n",
      "   macro avg       0.55      0.55      0.55      1169\n",
      "weighted avg       0.64      0.66      0.65      1169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 66.81%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.05      0.09       175\n",
      "           1       0.65      0.96      0.77       622\n",
      "           2       0.75      0.47      0.57       372\n",
      "\n",
      "    accuracy                           0.67      1169\n",
      "   macro avg       0.73      0.49      0.48      1169\n",
      "weighted avg       0.70      0.67      0.61      1169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Train a Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Naive Bayes Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 65.53%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.03      0.07       175\n",
      "           1       0.65      0.94      0.77       622\n",
      "           2       0.68      0.48      0.56       372\n",
      "\n",
      "    accuracy                           0.66      1169\n",
      "   macro avg       0.69      0.48      0.46      1169\n",
      "weighted avg       0.67      0.66      0.60      1169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train a K-Nearest Neighbors model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=70)\n",
    "knn_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make y_pred\n",
    "y_pred = knn_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"KNN Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dense_if_sparse(X):\n",
    "    if hasattr(X, 'toarray'):\n",
    "        return X.toarray()\n",
    "    return X\n",
    "\n",
    "class SVM_classifier():\n",
    "    def __init__(self, learning_rate=0.01, lambda_=10, iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_ = lambda_\n",
    "        self.iterations = iterations\n",
    "        self.W = None\n",
    "        self.B = None\n",
    "    \n",
    "    def compute_hingle_loss(self, W, B, X_batch, y_batch):\n",
    "        \"\"\" calculate hinge loss \"\"\"\n",
    "        N = X_batch.shape[0]\n",
    "        distance = []\n",
    "        for idx, x in enumerate(X_batch):\n",
    "            distance.append(max(0, 1 - y_batch[idx] * (np.dot(x, W) + B)))\n",
    "\n",
    "        distances = np.array(distance) # let distance into numpy array \n",
    "    \n",
    "        hinge_loss = self.lambda_ * (np.sum(distances) / N) # find hinge loss\n",
    "        \n",
    "        # calculate cost\n",
    "        cost = 1 / 2 * np.dot(W, W) + hinge_loss\n",
    "        return cost\n",
    "\n",
    "    def gradientDescent(self, W, B, X_batch, Y_batch):\n",
    "        distance = []\n",
    "        for idx, x in enumerate(X_batch):\n",
    "            distance.append(1 - Y_batch[idx] * (np.dot(x, W) + B))\n",
    "    \n",
    "        dw = np.zeros(len(W))\n",
    "        dB = 0\n",
    "        for idx, d in enumerate(distance):\n",
    "            if max(0, d) == 0:\n",
    "                dw += W\n",
    "                dB += 0\n",
    "            else:\n",
    "                dw += W - (self.lambda_ * Y_batch[idx] * X_batch[idx])\n",
    "                dB += 0 - (self.lambda_ * Y_batch[idx])\n",
    "        \n",
    "        dw = dw / len(Y_batch)  # average\n",
    "        dB = dB / len(Y_batch)  # avg\n",
    "        return dw, dB\n",
    "        \n",
    "    def fit(self, features, outputs) -> bool:\n",
    "        # print(features)\n",
    "        features = convert_to_dense_if_sparse(features)\n",
    "\n",
    "        # print(features.shape)\n",
    "        max_epochs = self.iterations\n",
    "        weights = np.zeros(features.shape[1])\n",
    "        bias = 0\n",
    "        nth = 0\n",
    "\n",
    "        prev_cost = float(\"inf\")\n",
    "        cost_threshold = 0.01  # in percent\n",
    "        \n",
    "        for epoch in range(1, max_epochs):\n",
    "            gradW, gradB = self.gradientDescent(weights, bias, features, outputs)\n",
    "\n",
    "            # convergence check on 2^nth epoch\n",
    "            if epoch == 2 ** nth or epoch == max_epochs - 1:\n",
    "                cost = self.compute_hingle_loss(weights, bias, features, outputs)\n",
    "                print(\"Epoch is:{} and Cost is: {}\".format(epoch, cost))\n",
    "                # stoppage criterion\n",
    "                if abs(prev_cost - cost) < cost_threshold * prev_cost:\n",
    "                    self.W = weights\n",
    "                    self.B = bias\n",
    "                    return True\n",
    "                prev_cost = cost\n",
    "                nth += 1\n",
    "            \n",
    "            # update grad\n",
    "            weights = weights - (self.learning_rate * gradW)\n",
    "            bias = bias - (self.learning_rate * gradB)\n",
    "            \n",
    "        self.W = weights\n",
    "        self.B = bias\n",
    "        return True\n",
    "    \n",
    "    def decisionFunc(self, X):\n",
    "        X = convert_to_dense_if_sparse(X)\n",
    "        ans = []\n",
    "        for x in X:\n",
    "            ans.append(np.dot(x, self.W) + self.B)\n",
    "        return np.array(ans)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = convert_to_dense_if_sparse(X)\n",
    "        # print(X)\n",
    "        prediction = []\n",
    "        for x in X:\n",
    "            prediction.append(np.dot(x, self.W) + self.B) # w.x + b\n",
    "        \n",
    "        # print(np.sign(prediction))\n",
    "        return np.sign(prediction)\n",
    "\n",
    "    # Evaluate the model\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        predictions = self.predict(X_test)\n",
    "        correct = 0\n",
    "        cnt_pos = 0\n",
    "        cnt_neg = 0\n",
    "        for i in range(predictions.shape[0]):\n",
    "            if predictions[i] == y_test[i]:\n",
    "                correct += 1\n",
    "            \n",
    "            if y_test[i] == 1:\n",
    "                cnt_pos += 1\n",
    "            else:\n",
    "                cnt_neg += 1\n",
    "        accuracy = correct / y_test.shape[0]\n",
    "        print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "        print(f\"Pos_rate: {cnt_pos / y_test.shape[0] * 100:.2}%\")\n",
    "        return accuracy\n",
    "    \n",
    "\n",
    "class OvRClassifier:\n",
    "    def __init__(self, n_classes, learning_rate=0.001, lambda_=10000, iterations=1000):\n",
    "        self.n_classes = n_classes\n",
    "        self.models = [SVM_classifier(learning_rate, lambda_, iterations) for _ in range(n_classes)]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n_classes):\n",
    "            y_binary = np.where(y == i, 1, -1) # in y and val == i then in y_binary = 1, others = -1 \n",
    "            self.models[i].fit(X, y_binary)\n",
    "\n",
    "    def predict(self, X):\n",
    "        decision_values = np.array([model.decisionFunc(X) for model in self.models])\n",
    "        return np.argmax(decision_values, axis=0)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        predictions = self.predict(X_test)\n",
    "        correct = 0\n",
    "        cnt_pos = 0\n",
    "        cnt_neg = 0\n",
    "        cnt_net = 0\n",
    "        for i in range(predictions.shape[0]):\n",
    "            if predictions[i] == y_test[i]:\n",
    "                correct += 1\n",
    "            \n",
    "            if y_test[i] == 2:\n",
    "                cnt_pos += 1\n",
    "            elif y_test[i] == 0:\n",
    "                cnt_neg += 1\n",
    "            else:\n",
    "                cnt_net += 1\n",
    "                \n",
    "        accuracy = correct / y_test.shape[0]\n",
    "        print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "        print(f\"Number: {cnt_pos}, {cnt_neg}, {cnt_net}\")\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = OvRClassifier(n_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is:1 and Cost is: 10000.0\n",
      "Epoch is:2 and Cost is: 11942.179702986883\n",
      "Epoch is:4 and Cost is: 7543.963381272171\n",
      "Epoch is:8 and Cost is: 11877.288286360781\n",
      "Epoch is:16 and Cost is: 7745.318347159508\n",
      "Epoch is:32 and Cost is: 12359.092777590484\n",
      "Epoch is:64 and Cost is: 3867.706701647922\n",
      "Epoch is:128 and Cost is: 2984.945406229369\n",
      "Epoch is:256 and Cost is: 2919.3158522412864\n",
      "Epoch is:512 and Cost is: 2541.088397144313\n",
      "Epoch is:999 and Cost is: 3030.508436172383\n",
      "Epoch is:1 and Cost is: 10000.0\n",
      "Epoch is:2 and Cost is: 9349.060194380203\n",
      "Epoch is:4 and Cost is: 22501.723882930546\n",
      "Epoch is:8 and Cost is: 14498.441942188265\n",
      "Epoch is:16 and Cost is: 9630.768945856384\n",
      "Epoch is:32 and Cost is: 20201.362792690663\n",
      "Epoch is:64 and Cost is: 15784.08380024346\n",
      "Epoch is:128 and Cost is: 16765.80851846506\n",
      "Epoch is:256 and Cost is: 14298.77589198786\n",
      "Epoch is:512 and Cost is: 10999.593209766615\n",
      "Epoch is:999 and Cost is: 8024.036871774681\n",
      "Epoch is:1 and Cost is: 10000.0\n",
      "Epoch is:2 and Cost is: 14878.83677144639\n",
      "Epoch is:4 and Cost is: 16420.614031804842\n",
      "Epoch is:8 and Cost is: 11192.417225292864\n",
      "Epoch is:16 and Cost is: 11796.574255969052\n",
      "Epoch is:32 and Cost is: 8480.913642428743\n",
      "Epoch is:64 and Cost is: 18210.157243029622\n",
      "Epoch is:128 and Cost is: 8160.865336923726\n",
      "Epoch is:256 and Cost is: 15160.87476818362\n",
      "Epoch is:512 and Cost is: 13423.657947173688\n",
      "Epoch is:999 and Cost is: 3483.490585410297\n"
     ]
    }
   ],
   "source": [
    "svm.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.89%\n",
      "Number: 372, 175, 622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6988879384088965"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.evaluate(X_test_tfidf, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
