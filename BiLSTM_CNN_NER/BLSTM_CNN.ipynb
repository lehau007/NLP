{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CustomLSTMCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.forget_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.output_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.cell_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "        combined = torch.cat((x, h_prev), 1)   # concatinate two matrices\n",
    "        \n",
    "        f_t = torch.sigmoid(self.forget_gate(combined))\n",
    "        i_t = torch.sigmoid(self.input_gate(combined))\n",
    "        o_t = torch.sigmoid(self.output_gate(combined))\n",
    "        c_tilde = torch.tanh(self.cell_gate(combined))\n",
    "\n",
    "        c_t = f_t * c_prev + i_t * c_tilde\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "        return h_t, c_t\n",
    "\n",
    "class CustomBLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CustomBLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_cell_fwd = CustomLSTMCell(input_size, hidden_size)\n",
    "        self.lstm_cell_bwd = CustomLSTMCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Initialize hidden and cell states\n",
    "        h_fwd = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        c_fwd = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        h_bwd = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        c_bwd = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "\n",
    "        # Forward LSTM \n",
    "        outputs_fwd = []\n",
    "        for t in range(seq_len):\n",
    "            h_fwd, c_fwd = self.lstm_cell_fwd(x[:, t, :], (h_fwd, c_fwd))\n",
    "            outputs_fwd.append(h_fwd)\n",
    "\n",
    "        # Backward LSTM \n",
    "        outputs_bwd = []\n",
    "        for t in reversed(range(seq_len)):\n",
    "            h_bwd, c_bwd = self.lstm_cell_bwd(x[:, t, :], (h_bwd, c_bwd))\n",
    "            outputs_bwd.append(h_bwd)\n",
    "        \n",
    "        # Concatenate forward and backward\n",
    "        outputs_bwd.reverse()  # Match idx with forward\n",
    "        outputs = torch.cat([torch.stack(outputs_fwd, dim=1), torch.stack(outputs_bwd, dim=1)], dim=2)\n",
    "        \n",
    "        return outputs  # Shape of output is: (batch_size, seq_len, hidden_size * 2)\n",
    "\n",
    "class BLSTMCNNNER(nn.Module):\n",
    "    def __init__(self, word_vocab_size, word_embedding_dim, char_vocab_size, char_embedding_dim,\n",
    "                 char_out_channels, char_kernel_size, hidden_dim, output_dim, dropout, learning_rate=1e-3):\n",
    "        super(BLSTMCNNNER, self).__init__()\n",
    "\n",
    "        # Word-level Embeddings\n",
    "        self.word_embedding = nn.Embedding(word_vocab_size, word_embedding_dim)\n",
    "        \n",
    "        # Character-level CNN Embeddings\n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
    "        self.conv = nn.Conv1d(char_embedding_dim, char_out_channels, char_kernel_size, padding=1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        # BLSTM\n",
    "        self.blstm = CustomBLSTM(word_embedding_dim + char_out_channels, hidden_dim)\n",
    "        \n",
    "        # Create a fully connected layer in this to convert out ouput of bLSTM to one hot vector size for prediction\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 256)\n",
    "        self.fc_out = nn.Linear(256, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "        # Create optimizer and Loss function\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.NLLLoss(ignore_index=-1) \n",
    "\n",
    "    def forward(self, word_inputs, char_inputs):\n",
    "        batch_size, seq_len = word_inputs.size()\n",
    "        \n",
    "        # Word Embeddings\n",
    "        word_embeds = self.word_embedding(word_inputs)\n",
    "        \n",
    "        # Character-level CNN Embedding\n",
    "        char_embeds = self.char_embedding(char_inputs.view(-1, char_inputs.size(-1)))\n",
    "        char_embeds = char_embeds.permute(0, 2, 1)  # for Conv1d input format\n",
    "        char_out = F.relu(self.conv(char_embeds))\n",
    "        char_out = self.max_pool(char_out).squeeze(-1)\n",
    "        char_out = char_out.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Concatenate word and character embeddings\n",
    "        embeds = torch.cat((word_embeds, char_out), dim=-1)\n",
    "        embeds = self.dropout(embeds)\n",
    "        \n",
    "        # print(embeds.shape)\n",
    "\n",
    "        # BLSTM\n",
    "        lstm_out = self.blstm(embeds)  # (batch_size, seq_len, hidden_dim * 2)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        tag_scores = self.log_softmax(self.fc_out(self.dropout(self.fc(lstm_out))))  # (batch_size, seq_len, output_dim)\n",
    "        \n",
    "        return tag_scores\n",
    "\n",
    "    def fit(self, train_dataloader, val_dataloader, num_epochs=5, clip=1.0):\n",
    "        self.train()\n",
    "\n",
    "        # self.evaluate(val_dataloader)\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for word_inputs, char_inputs, labels in train_dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.forward(word_inputs, char_inputs)\n",
    "                \n",
    "                # Reshape outputs and labels for NLLLoss\n",
    "                outputs = outputs.view(-1, outputs.shape[-1])  # Flatten for loss calculation\n",
    "                labels = labels.view(-1)  # Flatten target labels\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                ### optimize\n",
    "                loss.backward() # Back propagation\n",
    "                nn.utils.clip_grad_norm_(self.parameters(), clip) \n",
    "                self.optimizer.step()  # Gradient descent\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(train_dataloader)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # validation\n",
    "            self.evaluate(val_dataloader) \n",
    "\n",
    "    def evaluate(self, test_dataloader):\n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for word_inputs, char_inputs, labels in test_dataloader:\n",
    "                outputs = self.forward(word_inputs, char_inputs)\n",
    "\n",
    "                _, predicted = torch.max(outputs, dim=-1)  # Get predicted labels\n",
    "                \n",
    "                # print(predicted, hau) \n",
    "                for i, label in enumerate(labels):\n",
    "                    for j, alabel in enumerate(label):\n",
    "                        if alabel != -1 and alabel != 0:\n",
    "                            total += 1\n",
    "                            if alabel == predicted[i, j]:\n",
    "                                correct += 1\n",
    "        \n",
    "        accuracy = correct / total * 100\n",
    "        print(f\"Correct: {correct}, Total: {total}, Accuracy: {accuracy:.2f}%\")\n",
    "        return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('ner_dataset.csv', encoding= 'unicode_escape')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_map(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['Word'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['Tag'].to_list()))\n",
    "    \n",
    "    idx2tok = {idx + 1:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx + 1 for  idx, tok in enumerate(vocab)}\n",
    "\n",
    "    # PADDING AND UNK\n",
    "    tok2idx['PAD'] = 0\n",
    "    idx2tok[0] = 'PAD' \n",
    "    tok2idx['UNK'] = len(vocab) + 1\n",
    "    idx2tok[len(vocab) + 1] = 'UNK'\n",
    "    \n",
    "    return tok2idx, idx2tok\n",
    "\n",
    "token2idx, idx2token = get_dict_map(data, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(data, 'tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>15369</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>23521</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>17526</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>20967</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>32354</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag  Word_idx  Tag_idx\n",
       "0  Sentence: 1      Thousands  NNS   O     15369       11\n",
       "1          NaN             of   IN   O     23521       11\n",
       "2          NaN  demonstrators  NNS   O     17526       11\n",
       "3          NaN           have  VBP   O     20967       11\n",
       "4          NaN        marched  VBN   O     32354       11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Word_idx'] = data['Word'].map(token2idx)\n",
    "data['Tag_idx'] = data['Tag'].map(tag2idx)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count tag_idx, type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>NaN</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "      <td>34010</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>NaN</td>\n",
       "      <td>responded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "      <td>8236</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "      <td>11396</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "      <td>702</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>NaN</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>281</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentence #       Word  POS Tag  Word_idx  Tag_idx\n",
       "1048570        NaN       they  PRP   O     34010       11\n",
       "1048571        NaN  responded  VBD   O      8236       11\n",
       "1048572        NaN         to   TO   O     11396       11\n",
       "1048573        NaN        the   DT   O       702       11\n",
       "1048574        NaN     attack   NN   O       281       11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: O - idx: 11 cnt: 887908\n",
      "Tag: B-geo - idx: 5 cnt: 37644\n",
      "Tag: B-gpe - idx: 12 cnt: 15870\n",
      "Tag: B-per - idx: 10 cnt: 16990\n",
      "Tag: I-geo - idx: 15 cnt: 7414\n",
      "Tag: B-org - idx: 17 cnt: 20143\n",
      "Tag: I-org - idx: 9 cnt: 16784\n",
      "Tag: B-tim - idx: 16 cnt: 20333\n",
      "Tag: B-art - idx: 14 cnt: 402\n",
      "Tag: I-art - idx: 6 cnt: 297\n",
      "Tag: I-per - idx: 13 cnt: 17251\n",
      "Tag: I-gpe - idx: 2 cnt: 198\n",
      "Tag: I-tim - idx: 1 cnt: 6528\n",
      "Tag: B-nat - idx: 3 cnt: 201\n",
      "Tag: B-eve - idx: 7 cnt: 308\n",
      "Tag: I-eve - idx: 4 cnt: 253\n",
      "Tag: I-nat - idx: 8 cnt: 51\n"
     ]
    }
   ],
   "source": [
    "cnt = {}\n",
    "for x in data['Tag_idx']:\n",
    "    if x not in cnt:\n",
    "        cnt[x] = 1\n",
    "    else:\n",
    "        cnt[x] += 1\n",
    "\n",
    "for item in cnt.items():\n",
    "    print(\"Tag:\", idx2tag[item[0]], \"- idx:\", item[0], \"cnt:\", item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msilaptop\\AppData\\Local\\Temp\\ipykernel_7908\\1728395765.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_fillna = data.fillna(method='ffill', axis=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>15369</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>23521</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>17526</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>20967</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>32354</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag  Word_idx  Tag_idx\n",
       "0  Sentence: 1      Thousands  NNS   O     15369       11\n",
       "1  Sentence: 1             of   IN   O     23521       11\n",
       "2  Sentence: 1  demonstrators  NNS   O     17526       11\n",
       "3  Sentence: 1           have  VBP   O     20967       11\n",
       "4  Sentence: 1        marched  VBN   O     32354       11"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill na\n",
    "data_fillna = data.fillna(method='ffill', axis=0)\n",
    "data_fillna.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
       "      <td>[15369, 23521, 17526, 20967, 32354, 13001, 319...</td>\n",
       "      <td>[11, 11, 11, 11, 11, 11, 5, 11, 11, 11, 11, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 10</td>\n",
       "      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n",
       "      <td>[JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...</td>\n",
       "      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[1794, 24359, 11844, 34010, 10016, 11396, 231,...</td>\n",
       "      <td>[12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 100</td>\n",
       "      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n",
       "      <td>[NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...</td>\n",
       "      <td>[O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...</td>\n",
       "      <td>[22017, 18741, 31394, 30590, 5851, 27676, 2246...</td>\n",
       "      <td>[11, 11, 16, 11, 11, 11, 11, 11, 5, 11, 11, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1000</td>\n",
       "      <td>[They, left, after, a, tense, hour-long, stand...</td>\n",
       "      <td>[PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[2387, 13755, 1752, 22952, 29097, 19348, 9318,...</td>\n",
       "      <td>[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 10000</td>\n",
       "      <td>[U.N., relief, coordinator, Jan, Egeland, said...</td>\n",
       "      <td>[NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...</td>\n",
       "      <td>[B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...</td>\n",
       "      <td>[15411, 6692, 12937, 34136, 6361, 9787, 21920,...</td>\n",
       "      <td>[5, 11, 11, 10, 13, 11, 16, 11, 5, 11, 12, 11,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentence #                                               Word  \\\n",
       "0      Sentence: 1  [Thousands, of, demonstrators, have, marched, ...   \n",
       "1     Sentence: 10  [Iranian, officials, say, they, expect, to, ge...   \n",
       "2    Sentence: 100  [Helicopter, gunships, Saturday, pounded, mili...   \n",
       "3   Sentence: 1000  [They, left, after, a, tense, hour-long, stand...   \n",
       "4  Sentence: 10000  [U.N., relief, coordinator, Jan, Egeland, said...   \n",
       "\n",
       "                                                 POS  \\\n",
       "0  [NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...   \n",
       "1  [JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...   \n",
       "2  [NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...   \n",
       "3     [PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]   \n",
       "4  [NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...   \n",
       "\n",
       "                                                 Tag  \\\n",
       "0  [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...   \n",
       "1  [B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "2  [O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...   \n",
       "3                  [O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4  [B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...   \n",
       "\n",
       "                                            Word_idx  \\\n",
       "0  [15369, 23521, 17526, 20967, 32354, 13001, 319...   \n",
       "1  [1794, 24359, 11844, 34010, 10016, 11396, 231,...   \n",
       "2  [22017, 18741, 31394, 30590, 5851, 27676, 2246...   \n",
       "3  [2387, 13755, 1752, 22952, 29097, 19348, 9318,...   \n",
       "4  [15411, 6692, 12937, 34136, 6361, 9787, 21920,...   \n",
       "\n",
       "                                             Tag_idx  \n",
       "0  [11, 11, 11, 11, 11, 11, 5, 11, 11, 11, 11, 11...  \n",
       "1  [12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1...  \n",
       "2  [11, 11, 16, 11, 11, 11, 11, 11, 5, 11, 11, 11...  \n",
       "3       [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]  \n",
       "4  [5, 11, 11, 10, 13, 11, 16, 11, 5, 11, 12, 11,...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by\n",
    "data_group = data_fillna.groupby('Sentence #', as_index=False).agg(list)\n",
    "data_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, dataframe, char_vocab, max_word_len=10):\n",
    "        self.dataframe = dataframe\n",
    "        self.char_vocab = char_vocab\n",
    "        self.max_word_len = max_word_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        \n",
    "        # Word-level indices\n",
    "        word_indices = torch.tensor(row['Word_idx'], dtype=torch.long)\n",
    "        \n",
    "        # Character-level indices\n",
    "        char_indices = [\n",
    "            [self.char_vocab.get(char, 0) for char in word[:self.max_word_len]]\n",
    "            for word in row['Word']\n",
    "        ]\n",
    "        char_indices = [chars + [0] * (self.max_word_len - len(chars)) for chars in char_indices]\n",
    "        char_indices = torch.tensor(char_indices, dtype=torch.long)\n",
    "        \n",
    "        # Tag labels\n",
    "        tag_indices = torch.tensor(row['Tag_idx'], dtype=torch.long)\n",
    "        \n",
    "        return word_indices, char_indices, tag_indices\n",
    "\n",
    "def collate_fn(batch):\n",
    "    word_inputs = [item[0] for item in batch]\n",
    "    char_inputs = [item[1] for item in batch]\n",
    "    labels = [item[2] for item in batch]\n",
    "    \n",
    "    # Pad word and label sequences to the same length\n",
    "    word_inputs = torch.nn.utils.rnn.pad_sequence(word_inputs, batch_first=True, padding_value=0)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-1)\n",
    "    \n",
    "    # Pad character sequences to match word lengths\n",
    "    max_word_len = char_inputs[0].size(1)\n",
    "    char_inputs = [F.pad(chars, (0, 0, 0, word_inputs.size(1) - chars.size(0))) for chars in char_inputs]\n",
    "    char_inputs = torch.stack(char_inputs)\n",
    "    \n",
    "    return word_inputs, char_inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "char_vocab = {}\n",
    "i = 1\n",
    "for val in range(ord('a'), ord('z') + 1):\n",
    "    char_vocab[chr(val)] = i\n",
    "    i += 1\n",
    "\n",
    "for val in range(ord('A'), ord('Z') + 1):\n",
    "    char_vocab[chr(val)] = i\n",
    "    i += 1\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data = train_test_split(data_group, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = NERDataset(train_data, char_vocab)\n",
    "val_dataset = NERDataset(val_data, char_vocab)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52}\n"
     ]
    }
   ],
   "source": [
    "print(char_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.NERDataset object at 0x000001FB84E21EE0>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Loss: 0.3378\n",
      "Accuracy: 75.42%\n",
      "Epoch [2/4], Loss: 0.1205\n",
      "Accuracy: 81.81%\n",
      "Epoch [3/4], Loss: 0.0919\n",
      "Accuracy: 82.66%\n",
      "Epoch [4/4], Loss: 0.0754\n",
      "Accuracy: 83.60%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import torch_directml\n",
    "\n",
    "device = torch_directml.device()\n",
    "\n",
    "for word_inputs, char_inputs, labels in train_data_loader:\n",
    "    word_inputs, char_inputs, labels = word_inputs.to(device), char_inputs.to(device), labels.to(device)\n",
    "\n",
    "for word_inputs, char_inputs, labels in val_data_loader:\n",
    "    word_inputs, char_inputs, labels = word_inputs.to(device), char_inputs.to(device), labels.to(device)\n",
    "\"\"\"\n",
    "output_dim = max(data_group['Tag_idx'].explode()) + 1\n",
    "\n",
    "# Example usage with model\n",
    "model = BLSTMCNNNER(\n",
    "    word_vocab_size=50000,          # size of word vocabulary\n",
    "    word_embedding_dim=100,         # word embedding dimension\n",
    "    char_vocab_size=100,            # size of character vocabulary\n",
    "    char_embedding_dim=30,          # character embedding dimension\n",
    "    char_out_channels=50,           # number of CNN output channels for character embeddings\n",
    "    char_kernel_size=3,             # kernel size for CNN over character embeddings\n",
    "    hidden_dim=128,                 # hidden size for BiLSTM\n",
    "    output_dim=output_dim,                  # number of output tags\n",
    "    dropout=0.5                     # dropout rate\n",
    ")\n",
    "\n",
    "# model = model.to(device)  \n",
    "\n",
    "# Training loop example\n",
    "model.fit(train_dataloader=train_data_loader, val_dataloader=val_data_loader, num_epochs=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
